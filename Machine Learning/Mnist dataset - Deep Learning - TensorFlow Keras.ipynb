{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Learning - TensorFlow Keras.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ah7sF6BOESGd","colab_type":"text"},"source":["# **Deep Learning - TensorFlow Keras**\n","\n","The mathematical challenge for the artificial neural network is to best optimize thousands or millions or whatever number of weights you have, so that your output layer results in what you were hoping for. Solving for this problem, and building out the layers of our neural network model is exactly what TensorFlow is for. TensorFlow is used for all things \"operations on tensors.\" A tensor in this case is nothing fancy. It's a multi-dimensional array.\n","\n","To install TensorFlow, simply do a:"]},{"cell_type":"code","metadata":{"id":"fVQ832Ba93Cq","colab_type":"code","colab":{}},"source":["!pip install --upgrade tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3XTvn_o-QAe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aec0bdac-f80c-4ea1-f167-bb16521c8f86","executionInfo":{"status":"ok","timestamp":1558270336454,"user_tz":-120,"elapsed":6740,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","tf.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.13.1'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Hsp7lxCRFa09","colab_type":"text"},"source":["Once we've got tensorflow imported, we can then begin to prepare our data, model it, and then train it. For the sake of simplicity, we'll be using the most common \"hello world\" example for deep learning, which is the mnist dataset. It's a dataset of hand-written digits, 0 through 9. It's 28x28 images of these hand-written digits. We will show an example of using outside data as well, but, for now, let's load in this data:"]},{"cell_type":"code","metadata":{"id":"H89gAwGm-deS","colab_type":"code","colab":{}},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfkKnIKWGSIf","colab_type":"text"},"source":["When you're working with your own collected data, chances are, it wont be packaged up so nicely, and you'll spend a bit more time and effort on this step. But, for now, woo!\n","\n","What exactly do we have here? Let's take a quick peak.\n","\n","So the x_train data is the \"features.\" In this case, the features are pixel values of the 28x28 images of these digits 0-9. The y_train is the label (is it a 0,1,2,3,4,5,6,7,8 or a 9?)\n","\n","The testing variants of these variables is the \"out of sample\" examples that we will use. These are examples from our data that we're going to set aside, reserving them for testing the model.\n","\n","Neural networks are exceptionally good at fitting to data, so much so that they will commonly over-fit the data. Our real hope is that the neural network doesn't just memorize our data and that it instead \"generalizes\" and learns the actual problem and patterns associated with it.\n","\n","Let's look at this actual data:"]},{"cell_type":"code","metadata":{"id":"hPg0THQoGO0q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":969},"outputId":"9cdb962b-cffb-4ed6-d494-49211f75244d","executionInfo":{"status":"ok","timestamp":1558270336812,"user_tz":-120,"elapsed":7088,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["print(x_train[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n","  175  26 166 255 247 127   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n","  225 172 253 242 195  64   0   0   0   0]\n"," [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n","   93  82  82  56  39   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n","   25   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n","  150  27   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n","  253 187   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n","  253 249  64   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n","  253 207   2   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n","  250 182   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n","   78   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TOrvCBFgGbzU","colab_type":"text"},"source":["Lets visualise this."]},{"cell_type":"code","metadata":{"id":"fexUX6zqGZ2P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"fd2f2430-13f5-4d12-89e8-fc6dce6f6912","executionInfo":{"status":"ok","timestamp":1558270336969,"user_tz":-120,"elapsed":7244,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["plt.imshow(x_train[0],cmap=plt.cm.binary)\n","plt.show()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWArzGoGmwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"FqhH2byHGkL2","colab_type":"text"},"source":["Okay, that makes sense. How about the value for y_train with the same index?"]},{"cell_type":"code","metadata":{"id":"W0luuyIlGlKv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"21ba0947-c8ca-4bd4-bd37-3fc2800a7947","executionInfo":{"status":"ok","timestamp":1558270336973,"user_tz":-120,"elapsed":7247,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["print(y_train[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pxt5BcR2Gt2h","colab_type":"text"},"source":["It's generally a good idea to \"normalize\" your data. This typically involves scaling the data to be between 0 and 1, or maybe -1 and positive 1. In our case, each \"pixel\" is a feature, and each feature currently ranges from 0 to 255. Not quite 0 to 1. Let's change that with a handy utility function:"]},{"cell_type":"code","metadata":{"id":"LeeskSRPGrZ_","colab_type":"code","colab":{}},"source":["x_train = tf.keras.utils.normalize(x_train, axis=1)\n","x_test = tf.keras.utils.normalize(x_test, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12Hq6_a7G1a9","colab_type":"text"},"source":["Let's peak one more time:"]},{"cell_type":"code","metadata":{"id":"CeQSaZf1G0wl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2649},"outputId":"fd293fe0-6ecf-44c3-e62f-3e5042dd8453","executionInfo":{"status":"ok","timestamp":1558270337889,"user_tz":-120,"elapsed":8158,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["print(x_train[0])\n","\n","plt.imshow(x_train[0],cmap=plt.cm.binary)\n","plt.show()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n","  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n","  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n","  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n","  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n","  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n","  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n","  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.0253731  0.00171577 0.22713296\n","  0.33153488 0.11664776 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.20500962\n","  0.33153488 0.24625638 0.00291174 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.01622378\n","  0.24897876 0.32790981 0.10191096 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n","  0.04089933 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n","  0.245396   0.05882702 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.02333517 0.12857881 0.32549285\n","  0.41390126 0.40743158 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.32161793\n","  0.41390126 0.54251585 0.20001074 0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n","  0.41390126 0.45100715 0.00625034 0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n","  0.40899334 0.39653769 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.04117838 0.16813739\n","  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n","  0.12760592 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n","  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n","  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.37491383 0.56222061\n","  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n","  0.17428513 0.01425695 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.92705966 0.82698729\n","  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADltJREFUeJzt3W+MVfWdx/HPF5hBHRoBGSb8GRiW\nmFWCWag3IwGzYVNpLGnEPjElpmETU2pSk5L0wRr7oDw0zbaNiZsqXUnRdKWbtEYSyW6VNCFNVmQ0\nKFosIAwyODJDBv/wJ1aH7z6YQzPqnN8Z779zh+/7lUzm3vM9555vDnzm3Ht/956fubsAxDOt7AYA\nlIPwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IakYzdzZv3jzv6elp5i6BUPr7+3Xu3DmbzLo1\nhd/M7pb0mKTpkv7T3R9Nrd/T06O+vr5adgkgoVKpTHrdqp/2m9l0Sf8h6VuSVkjabGYrqn08AM1V\ny2v+XknH3f2Eu/9N0m5Jm+rTFoBGqyX8iySdHnd/IFv2OWa21cz6zKxveHi4ht0BqKeGv9vv7jvc\nveLulc7OzkbvDsAk1RL+M5K6x91fnC0DMAXUEv6Dkm42s2Vm1i7pu5L21KctAI1W9VCfu39mZg9J\n+l+NDfXtdPe36tYZgIaqaZzf3fdK2lunXgA0ER/vBYIi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEBThB4Ii/EBQhB8IivADQRF+IKiaZuk1s35JH0salfSZu1fq0RTqx92T9U8//bSm7YscOXKk6m1P\nnTqVrK9fvz5Z3759e27twIEDyW3Pnz+frPf39yfrly9fTtZbQU3hz/yLu5+rw+MAaCKe9gNB1Rp+\nl/RHM3vVzLbWoyEAzVHr0/473f2Mmc2X9KKZve3u+8evkP1R2CpJS5YsqXF3AOqlpjO/u5/Jfg9J\nek5S7wTr7HD3irtXOjs7a9kdgDqqOvxm1mFmX7t6W9I3Jb1Zr8YANFYtT/u7JD1nZlcf57/c/X/q\n0hWAhqs6/O5+QtI/1bGXa9aHH36YrI+Ojibr7733XrI+MjKSW8v+OOc6ffp0sn7x4sVkvUhbW1tu\nrb29vaZ97969O1l/4YUXcmtLly5Nbtvd3Z2s33///cn6VMBQHxAU4QeCIvxAUIQfCIrwA0ERfiCo\nenyrL7yTJ08m688880xNjz9z5sxkffbs2bm1jo6O5LbTppX3979oGHLdunXJ+ieffJKsP/7447m1\nhQsXJrctOm7Lli1L1qcCzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/HVQdIWiG264IVm/dOlS\nPdupq/nz5yfrRV/LHR4ezq3NmJH+77dixYpkHbXhzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHO\nXwezZs1K1jdu3JisHz9+PFlfvHhxsn7w4MFkPWXOnDnJ+oYNG5L1orH6Dz74ILd29OjR5LZoLM78\nQFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU4Ti/me2U9G1JQ+6+Mls2V9LvJPVI6pd0n7ufb1ybU1vR\n99KXL1+erBddt//ChQu5tXfffTe57a233pqsF43jF0nNKdDb21vTY6M2kznz/0bS3V9Y9rCkfe5+\ns6R92X0AU0hh+N19v6SRLyzeJGlXdnuXpHvr3BeABqv2NX+Xuw9mt9+X1FWnfgA0Sc1v+Lm7S/K8\nupltNbM+M+tLXc8NQHNVG/6zZrZAkrLfQ3kruvsOd6+4e6XoQpcAmqfa8O+RtCW7vUXS8/VpB0Cz\nFIbfzJ6V9H+S/tHMBszsAUmPStpgZsck3ZXdBzCFFA7iuvvmnNI36txLWEXj+EWKrp2fUnQtgZ6e\nnqofG62NT/gBQRF+ICjCDwRF+IGgCD8QFOEHguLS3deASqWSW0t93VeShoZyP5wpSRoYGEjWiy4r\njtbFmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKc/xqQurz2mjVrktvu3bs3Wd+/f3+yvnDhwmS9\nqyv/8o5Flw1HY3HmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOe/xs2aNStZX7t2bbL+0ksvJevH\njh1L1vv7+3NrYzO95Vu6dGmy3tHRkawjjTM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVOM5vZjsl\nfVvSkLuvzJZtl/R9ScPZao+4e/qL4WhJRdfdv+eee5L1l19+OVlPzQtw6NCh5LaDg4PJ+u23356s\nz549O1mPbjJn/t9IunuC5b9091XZD8EHppjC8Lv7fkkjTegFQBPV8pr/ITN7w8x2mtmcunUEoCmq\nDf+vJC2XtErSoKSf561oZlvNrM/M+oaHh/NWA9BkVYXf3c+6+6i7X5H0a0m9iXV3uHvF3SudnZ3V\n9gmgzqoKv5ktGHf3O5LerE87AJplMkN9z0paL2memQ1I+qmk9Wa2SpJL6pf0gwb2CKABCsPv7psn\nWPxUA3pBC5o7d26yftdddyXrp0+fzq298soryW1ff/31ZP3w4cPJ+rZt25L16PiEHxAU4QeCIvxA\nUIQfCIrwA0ERfiAoLt2NmrS3tyfry5cvz60dPHiwpn0fPXo0WT9w4EBu7Y477qhp39cCzvxAUIQf\nCIrwA0ERfiAowg8ERfiBoAg/EBTj/EgaGUlfu/XEiRPJ+vnz53NrV65cqaqnqxYuXJis9/bmXmAK\n4swPhEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzn+N++ijj5L1ou/Ev/3228n65cuXk/W2trbcWtG1\nAKZNS5+bbrzxxmTdzJL16DjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQheP8ZtYt6WlJXZJc0g53\nf8zM5kr6naQeSf2S7nP3/C9vo2oXL15M1t95553c2smTJ2t67KJx/FrcdNNNyXrRtfVTcwKg2GTO\n/J9J+rG7r5C0RtIPzWyFpIcl7XP3myXty+4DmCIKw+/ug+7+Wnb7Y0lHJC2StEnSrmy1XZLubVST\nAOrvK73mN7MeSaslHZDU5e6DWel9jb0sADBFTDr8ZjZL0u8lbXP3z31g3N1dY+8HTLTdVjPrM7O+\n4eHhmpoFUD+TCr+ZtWks+L919z9ki8+a2YKsvkDS0ETbuvsOd6+4e6Wzs7MePQOog8Lw29hXo56S\ndMTdfzGutEfSluz2FknP1789AI0yma/0rpP0PUmHzexQtuwRSY9K+m8ze0DSKUn3NabFqe/ChQvJ\netHLoX379iXro6OjubWOjo7ktkVfmy0yf/78ZH316tW5tSVLltS0b9SmMPzu/mdJeV+M/kZ92wHQ\nLHzCDwiK8ANBEX4gKMIPBEX4gaAIPxAUl+6epNQlsJ944onktkVj6ZcuXUrWZ86cmazPnj07WU8p\n+tTl2rVrk/Xu7u5kffr06V+5JzQHZ34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCrMOP+TTz6ZrPf1\n9SXrAwMDubXrr78+ue0tt9ySrF933XXJepEZM/L/GVeuXJnc9rbbbkvWGae/dnHmB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgwozzP/jgg8n6okWLkvXU9el7enqq3lYqHmtva2tL1tesWZNba29vT26L\nuDjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQheP8ZtYt6WlJXZJc0g53f8zMtkv6vqSrk8s/4u57\nG9Vordy97BaAljKZD/l8JunH7v6amX1N0qtm9mJW+6W7/3vj2gPQKIXhd/dBSYPZ7Y/N7Iik9Mfh\nALS8r/Sa38x6JK2WdCBb9JCZvWFmO81sTs42W82sz8z6hoeHJ1oFQAkmHX4zmyXp95K2uftHkn4l\nabmkVRp7ZvDzibZz9x3uXnH3StG8cACaZ1LhN7M2jQX/t+7+B0ly97PuPuruVyT9WlJv49oEUG+F\n4Tczk/SUpCPu/otxyxeMW+07kt6sf3sAGmUy7/avk/Q9SYfN7FC27BFJm81slcaG//ol/aAhHQJo\niMm82/9nSTZBqWXH9AEU4xN+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ER\nfiAowg8ERfiBoKyZl7Q2s2FJp8YtmifpXNMa+GpatbdW7Uuit2rVs7el7j6p6+U1Nfxf2rlZn7tX\nSmsgoVV7a9W+JHqrVlm98bQfCIrwA0GVHf4dJe8/pVV7a9W+JHqrVim9lfqaH0B5yj7zAyhJKeE3\ns7vN7K9mdtzMHi6jhzxm1m9mh83skJn1ldzLTjMbMrM3xy2ba2Yvmtmx7PeE06SV1Nt2MzuTHbtD\nZraxpN66zexPZvYXM3vLzH6ULS/12CX6KuW4Nf1pv5lNl3RU0gZJA5IOStrs7n9paiM5zKxfUsXd\nSx8TNrN/lnRB0tPuvjJb9jNJI+7+aPaHc467/1uL9LZd0oWyZ27OJpRZMH5maUn3SvpXlXjsEn3d\npxKOWxln/l5Jx939hLv/TdJuSZtK6KPluft+SSNfWLxJ0q7s9i6N/edpupzeWoK7D7r7a9ntjyVd\nnVm61GOX6KsUZYR/kaTT4+4PqLWm/HZJfzSzV81sa9nNTKArmzZdkt6X1FVmMxMonLm5mb4ws3TL\nHLtqZryuN97w+7I73f3rkr4l6YfZ09uW5GOv2VppuGZSMzc3ywQzS/9dmceu2hmv662M8J+R1D3u\n/uJsWUtw9zPZ7yFJz6n1Zh8+e3WS1Oz3UMn9/F0rzdw80czSaoFj10ozXpcR/oOSbjazZWbWLum7\nkvaU0MeXmFlH9kaMzKxD0jfVerMP75G0Jbu9RdLzJfbyOa0yc3PezNIq+di13IzX7t70H0kbNfaO\n/zuSflJGDzl9/YOk17Oft8ruTdKzGnsa+KnG3ht5QNJNkvZJOibpJUlzW6i3ZyQdlvSGxoK2oKTe\n7tTYU/o3JB3KfjaWfewSfZVy3PiEHxAUb/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjq/wFP\nK1OkXgT91QAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mJLbcBY_G7ft","colab_type":"text"},"source":["Now let's build our model!"]},{"cell_type":"markdown","metadata":{"id":"8b8OvPqVHVOz","colab_type":"text"},"source":["A sequential model is what you're going to use most of the time. It just means things are going to go in direct order. A feed forward model."]},{"cell_type":"code","metadata":{"id":"LUwB-1OKHaId","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yo7nocf2Hjju","colab_type":"text"},"source":["Now, we'll pop in layers. Recall our neural network image? Was the input layer flat, or was it multi-dimensional? It was flat. So, we need to take this 28x28 image, and make it a flat 1x784. There are many ways for us to do this, but keras has a Flatten layer built just for us, so we'll use that."]},{"cell_type":"code","metadata":{"id":"gwNPi0tVHleO","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Flatten())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXiy00LQHuKr","colab_type":"text"},"source":["This will serve as our input layer. It's going to take the data we throw at it, and just flatten it for us. Next, we want our hidden layers. We're going to go with the simplest neural network layer, which is just a Dense layer. This refers to the fact that it's a densely-connected layer, meaning it's \"fully connected,\" where each node connects to each prior and subsequent node."]},{"cell_type":"code","metadata":{"id":"mo9Kbf1mHyaq","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lC6YRVK7Hx0-","colab_type":"text"},"source":["This layer has 128 units. The activation function is relu, short for rectified linear. Currently, relu is the activation function you should just default to.\n","Let's add another identical layer for good measure."]},{"cell_type":"code","metadata":{"id":"OrrakGPEIABK","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KFqMLFBICAu","colab_type":"text"},"source":["Now, we're ready for an output layer:"]},{"cell_type":"code","metadata":{"id":"j1S4VNCbIBnx","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vd0t52A6IN8W","colab_type":"text"},"source":["This is our final layer. It has 10 nodes. 1 node per possible number prediction. In this case, our activation function is a softmax function, since we're really actually looking for something more like a probability distribution of which of the possible prediction options this thing we're passing features through of is.\n","\n","Now we need to \"compile\" the model. This is where we pass the settings for actually optimizing/training the model we've defined."]},{"cell_type":"code","metadata":{"id":"vxjau7_NGIOW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"12f25e8b-133b-4470-c8ff-94e75ba69078","executionInfo":{"status":"ok","timestamp":1558270338171,"user_tz":-120,"elapsed":8403,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["model.compile(optimizer='adam',\n","             loss='sparse_categorical_crossentropy', \n","             metrics=['accuracy'])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uhMqNhMuIZvr","colab_type":"text"},"source":["Remember why we picked relu as an activation function? Same thing is true for the Adam optimizer. It's just a great default to start with.\n","\n","Next, we have our loss metric. Loss is a calculation of error. A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss. Again, there are many choices, but some form of categorical crossentropy is a good start for a classification task like this.\n","\n","Now, we fit!"]},{"cell_type":"code","metadata":{"id":"jnXqMoKQIcU-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"12f47f39-4f53-4b90-9f6d-7f72fe1cd43a","executionInfo":{"status":"ok","timestamp":1558270430957,"user_tz":-120,"elapsed":101186,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["model.fit(x_train, y_train, epochs=10)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","60000/60000 [==============================] - 9s 157us/sample - loss: 0.2604 - acc: 0.9228\n","Epoch 2/10\n","60000/60000 [==============================] - 9s 153us/sample - loss: 0.1054 - acc: 0.9686\n","Epoch 3/10\n","60000/60000 [==============================] - 9s 142us/sample - loss: 0.0718 - acc: 0.9775\n","Epoch 4/10\n","60000/60000 [==============================] - 9s 157us/sample - loss: 0.0530 - acc: 0.9829\n","Epoch 5/10\n","60000/60000 [==============================] - 9s 152us/sample - loss: 0.0407 - acc: 0.9865\n","Epoch 6/10\n","60000/60000 [==============================] - 9s 156us/sample - loss: 0.0303 - acc: 0.9902\n","Epoch 7/10\n","60000/60000 [==============================] - 9s 151us/sample - loss: 0.0264 - acc: 0.9915\n","Epoch 8/10\n","60000/60000 [==============================] - 9s 152us/sample - loss: 0.0208 - acc: 0.9927\n","Epoch 9/10\n","60000/60000 [==============================] - 9s 156us/sample - loss: 0.0179 - acc: 0.9941\n","Epoch 10/10\n","60000/60000 [==============================] - 10s 159us/sample - loss: 0.0169 - acc: 0.9941\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1907978a20>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"FXZllwFeImJ9","colab_type":"text"},"source":["As we train, we can see loss goes down (yay), and accuracy improves quite quickly to 98-99%\n","\n","Now that's loss and accuracy for in-sample data. Getting a high accuracy and low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). This is why we need to test on out-of-sample data (data we didn't use to train the model)."]},{"cell_type":"code","metadata":{"id":"3R4hr70IB9Yo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"bf8142ef-2188-4cdd-ab09-a67953e7f052","executionInfo":{"status":"ok","timestamp":1558270431660,"user_tz":-120,"elapsed":101850,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["val_loss, val_acc = model.evaluate(x_test, y_test)\n","print(val_loss, val_acc)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 0s 49us/sample - loss: 0.1053 - acc: 0.9745\n","0.10530238576161063 0.9745\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wE-S3ePX-tyO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2649},"outputId":"850b3821-616f-44aa-d05e-24fcb2c730fb","executionInfo":{"status":"ok","timestamp":1558270431943,"user_tz":-120,"elapsed":102130,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["plt.imshow(x_train[0], cmap = plt.cm.binary)\n","plt.show()\n","print(x_train[0])"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADltJREFUeJzt3W+MVfWdx/HPF5hBHRoBGSb8GRiW\nmFWCWag3IwGzYVNpLGnEPjElpmETU2pSk5L0wRr7oDw0zbaNiZsqXUnRdKWbtEYSyW6VNCFNVmQ0\nKFosIAwyODJDBv/wJ1aH7z6YQzPqnN8Z779zh+/7lUzm3vM9555vDnzm3Ht/956fubsAxDOt7AYA\nlIPwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IakYzdzZv3jzv6elp5i6BUPr7+3Xu3DmbzLo1\nhd/M7pb0mKTpkv7T3R9Nrd/T06O+vr5adgkgoVKpTHrdqp/2m9l0Sf8h6VuSVkjabGYrqn08AM1V\ny2v+XknH3f2Eu/9N0m5Jm+rTFoBGqyX8iySdHnd/IFv2OWa21cz6zKxveHi4ht0BqKeGv9vv7jvc\nveLulc7OzkbvDsAk1RL+M5K6x91fnC0DMAXUEv6Dkm42s2Vm1i7pu5L21KctAI1W9VCfu39mZg9J\n+l+NDfXtdPe36tYZgIaqaZzf3fdK2lunXgA0ER/vBYIi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEBThB4Ii/EBQhB8IivADQRF+IKiaZuk1s35JH0salfSZu1fq0RTqx92T9U8//bSm7YscOXKk6m1P\nnTqVrK9fvz5Z3759e27twIEDyW3Pnz+frPf39yfrly9fTtZbQU3hz/yLu5+rw+MAaCKe9gNB1Rp+\nl/RHM3vVzLbWoyEAzVHr0/473f2Mmc2X9KKZve3u+8evkP1R2CpJS5YsqXF3AOqlpjO/u5/Jfg9J\nek5S7wTr7HD3irtXOjs7a9kdgDqqOvxm1mFmX7t6W9I3Jb1Zr8YANFYtT/u7JD1nZlcf57/c/X/q\n0hWAhqs6/O5+QtI/1bGXa9aHH36YrI+Ojibr7733XrI+MjKSW8v+OOc6ffp0sn7x4sVkvUhbW1tu\nrb29vaZ97969O1l/4YUXcmtLly5Nbtvd3Z2s33///cn6VMBQHxAU4QeCIvxAUIQfCIrwA0ERfiCo\nenyrL7yTJ08m688880xNjz9z5sxkffbs2bm1jo6O5LbTppX3979oGHLdunXJ+ieffJKsP/7447m1\nhQsXJrctOm7Lli1L1qcCzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/HVQdIWiG264IVm/dOlS\nPdupq/nz5yfrRV/LHR4ezq3NmJH+77dixYpkHbXhzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHO\nXwezZs1K1jdu3JisHz9+PFlfvHhxsn7w4MFkPWXOnDnJ+oYNG5L1orH6Dz74ILd29OjR5LZoLM78\nQFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU4Ti/me2U9G1JQ+6+Mls2V9LvJPVI6pd0n7ufb1ybU1vR\n99KXL1+erBddt//ChQu5tXfffTe57a233pqsF43jF0nNKdDb21vTY6M2kznz/0bS3V9Y9rCkfe5+\ns6R92X0AU0hh+N19v6SRLyzeJGlXdnuXpHvr3BeABqv2NX+Xuw9mt9+X1FWnfgA0Sc1v+Lm7S/K8\nupltNbM+M+tLXc8NQHNVG/6zZrZAkrLfQ3kruvsOd6+4e6XoQpcAmqfa8O+RtCW7vUXS8/VpB0Cz\nFIbfzJ6V9H+S/tHMBszsAUmPStpgZsck3ZXdBzCFFA7iuvvmnNI36txLWEXj+EWKrp2fUnQtgZ6e\nnqofG62NT/gBQRF+ICjCDwRF+IGgCD8QFOEHguLS3deASqWSW0t93VeShoZyP5wpSRoYGEjWiy4r\njtbFmR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKc/xqQurz2mjVrktvu3bs3Wd+/f3+yvnDhwmS9\nqyv/8o5Flw1HY3HmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOe/xs2aNStZX7t2bbL+0ksvJevH\njh1L1vv7+3NrYzO95Vu6dGmy3tHRkawjjTM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVOM5vZjsl\nfVvSkLuvzJZtl/R9ScPZao+4e/qL4WhJRdfdv+eee5L1l19+OVlPzQtw6NCh5LaDg4PJ+u23356s\nz549O1mPbjJn/t9IunuC5b9091XZD8EHppjC8Lv7fkkjTegFQBPV8pr/ITN7w8x2mtmcunUEoCmq\nDf+vJC2XtErSoKSf561oZlvNrM/M+oaHh/NWA9BkVYXf3c+6+6i7X5H0a0m9iXV3uHvF3SudnZ3V\n9gmgzqoKv5ktGHf3O5LerE87AJplMkN9z0paL2memQ1I+qmk9Wa2SpJL6pf0gwb2CKABCsPv7psn\nWPxUA3pBC5o7d26yftdddyXrp0+fzq298soryW1ff/31ZP3w4cPJ+rZt25L16PiEHxAU4QeCIvxA\nUIQfCIrwA0ERfiAoLt2NmrS3tyfry5cvz60dPHiwpn0fPXo0WT9w4EBu7Y477qhp39cCzvxAUIQf\nCIrwA0ERfiAowg8ERfiBoAg/EBTj/EgaGUlfu/XEiRPJ+vnz53NrV65cqaqnqxYuXJis9/bmXmAK\n4swPhEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzn+N++ijj5L1ou/Ev/3228n65cuXk/W2trbcWtG1\nAKZNS5+bbrzxxmTdzJL16DjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQheP8ZtYt6WlJXZJc0g53\nf8zM5kr6naQeSf2S7nP3/C9vo2oXL15M1t95553c2smTJ2t67KJx/FrcdNNNyXrRtfVTcwKg2GTO\n/J9J+rG7r5C0RtIPzWyFpIcl7XP3myXty+4DmCIKw+/ug+7+Wnb7Y0lHJC2StEnSrmy1XZLubVST\nAOrvK73mN7MeSaslHZDU5e6DWel9jb0sADBFTDr8ZjZL0u8lbXP3z31g3N1dY+8HTLTdVjPrM7O+\n4eHhmpoFUD+TCr+ZtWks+L919z9ki8+a2YKsvkDS0ETbuvsOd6+4e6Wzs7MePQOog8Lw29hXo56S\ndMTdfzGutEfSluz2FknP1789AI0yma/0rpP0PUmHzexQtuwRSY9K+m8ze0DSKUn3NabFqe/ChQvJ\netHLoX379iXro6OjubWOjo7ktkVfmy0yf/78ZH316tW5tSVLltS0b9SmMPzu/mdJeV+M/kZ92wHQ\nLHzCDwiK8ANBEX4gKMIPBEX4gaAIPxAUl+6epNQlsJ944onktkVj6ZcuXUrWZ86cmazPnj07WU8p\n+tTl2rVrk/Xu7u5kffr06V+5JzQHZ34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCrMOP+TTz6ZrPf1\n9SXrAwMDubXrr78+ue0tt9ySrF933XXJepEZM/L/GVeuXJnc9rbbbkvWGae/dnHmB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgwozzP/jgg8n6okWLkvXU9el7enqq3lYqHmtva2tL1tesWZNba29vT26L\nuDjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQheP8ZtYt6WlJXZJc0g53f8zMtkv6vqSrk8s/4u57\nG9Vordy97BaAljKZD/l8JunH7v6amX1N0qtm9mJW+6W7/3vj2gPQKIXhd/dBSYPZ7Y/N7Iik9Mfh\nALS8r/Sa38x6JK2WdCBb9JCZvWFmO81sTs42W82sz8z6hoeHJ1oFQAkmHX4zmyXp95K2uftHkn4l\nabmkVRp7ZvDzibZz9x3uXnH3StG8cACaZ1LhN7M2jQX/t+7+B0ly97PuPuruVyT9WlJv49oEUG+F\n4Tczk/SUpCPu/otxyxeMW+07kt6sf3sAGmUy7/avk/Q9SYfN7FC27BFJm81slcaG//ol/aAhHQJo\niMm82/9nSTZBqWXH9AEU4xN+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ER\nfiAowg8ERfiBoKyZl7Q2s2FJp8YtmifpXNMa+GpatbdW7Uuit2rVs7el7j6p6+U1Nfxf2rlZn7tX\nSmsgoVV7a9W+JHqrVlm98bQfCIrwA0GVHf4dJe8/pVV7a9W+JHqrVim9lfqaH0B5yj7zAyhJKeE3\ns7vN7K9mdtzMHi6jhzxm1m9mh83skJn1ldzLTjMbMrM3xy2ba2Yvmtmx7PeE06SV1Nt2MzuTHbtD\nZraxpN66zexPZvYXM3vLzH6ULS/12CX6KuW4Nf1pv5lNl3RU0gZJA5IOStrs7n9paiM5zKxfUsXd\nSx8TNrN/lnRB0tPuvjJb9jNJI+7+aPaHc467/1uL9LZd0oWyZ27OJpRZMH5maUn3SvpXlXjsEn3d\npxKOWxln/l5Jx939hLv/TdJuSZtK6KPluft+SSNfWLxJ0q7s9i6N/edpupzeWoK7D7r7a9ntjyVd\nnVm61GOX6KsUZYR/kaTT4+4PqLWm/HZJfzSzV81sa9nNTKArmzZdkt6X1FVmMxMonLm5mb4ws3TL\nHLtqZryuN97w+7I73f3rkr4l6YfZ09uW5GOv2VppuGZSMzc3ywQzS/9dmceu2hmv662M8J+R1D3u\n/uJsWUtw9zPZ7yFJz6n1Zh8+e3WS1Oz3UMn9/F0rzdw80czSaoFj10ozXpcR/oOSbjazZWbWLum7\nkvaU0MeXmFlH9kaMzKxD0jfVerMP75G0Jbu9RdLzJfbyOa0yc3PezNIq+di13IzX7t70H0kbNfaO\n/zuSflJGDzl9/YOk17Oft8ruTdKzGnsa+KnG3ht5QNJNkvZJOibpJUlzW6i3ZyQdlvSGxoK2oKTe\n7tTYU/o3JB3KfjaWfewSfZVy3PiEHxAUb/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjq/wFP\nK1OkXgT91QAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[[0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n","  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n","  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n","  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n","  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n","  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n","  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n","  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.0253731  0.00171577 0.22713296\n","  0.33153488 0.11664776 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.20500962\n","  0.33153488 0.24625638 0.00291174 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.01622378\n","  0.24897876 0.32790981 0.10191096 0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n","  0.04089933 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n","  0.245396   0.05882702 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.02333517 0.12857881 0.32549285\n","  0.41390126 0.40743158 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.32161793\n","  0.41390126 0.54251585 0.20001074 0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n","  0.41390126 0.45100715 0.00625034 0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n","  0.40899334 0.39653769 0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.04117838 0.16813739\n","  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n","  0.12760592 0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n","  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n","  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.37491383 0.56222061\n","  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n","  0.17428513 0.01425695 0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.92705966 0.82698729\n","  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K5WAZWiJJGk2","colab_type":"text"},"source":["Finally, with your model, you can save it easily:"]},{"cell_type":"code","metadata":{"id":"EOcoYVe1CX73","colab_type":"code","colab":{}},"source":["model.save('epic_num_reader.model')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lE3hG__zJKfB","colab_type":"text"},"source":["Load it back:"]},{"cell_type":"code","metadata":{"id":"95TXdbeQCvkn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"14729f97-e315-4da6-d47f-20aea8d918c2","executionInfo":{"status":"ok","timestamp":1558270432172,"user_tz":-120,"elapsed":102261,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["new_model = tf.keras.models.load_model('epic_num_reader.model')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-9VsCnySJOgI","colab_type":"text"},"source":["finally, make predictions!"]},{"cell_type":"code","metadata":{"id":"0ZMl38xEC0xE","colab_type":"code","colab":{}},"source":["predictions = new_model.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOUr20xwJYCv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"12d9ae00-d406-48ce-c8a0-6d18c97eacbb","executionInfo":{"status":"ok","timestamp":1558270432703,"user_tz":-120,"elapsed":102734,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["print(predictions)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[[1.7076527e-12 1.4030441e-11 2.0785997e-09 ... 1.0000000e+00\n","  1.7244369e-12 2.7306463e-10]\n"," [1.4459032e-11 6.4040414e-08 9.9999964e-01 ... 2.9091372e-13\n","  1.8954451e-11 1.1611174e-16]\n"," [6.0609300e-12 9.9999893e-01 1.0325730e-08 ... 3.1408629e-08\n","  9.0256759e-07 2.4008368e-11]\n"," ...\n"," [1.8334614e-13 1.3029762e-09 2.6991313e-13 ... 1.0718118e-08\n","  3.8710577e-09 5.2992141e-07]\n"," [3.2287460e-20 1.2004707e-13 8.8719715e-20 ... 2.7385884e-15\n","  1.2493865e-11 7.2241721e-22]\n"," [2.8095278e-11 1.4112050e-12 1.4119608e-13 ... 2.0231817e-20\n","  1.0292719e-13 1.7657390e-15]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_0oMfON0Jc6v","colab_type":"text"},"source":["That sure doesn't start off as helpful, but recall these are probability distributions. We can get the actual number pretty simply:"]},{"cell_type":"code","metadata":{"id":"s_TgYmd0DQZ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2bc445e6-b50b-4125-89c0-70c314f2c4a7","executionInfo":{"status":"ok","timestamp":1558270432705,"user_tz":-120,"elapsed":102684,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["print(np.argmax(predictions[0]))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2mRAnpPlJgB_","colab_type":"text"},"source":["There's your prediction, let's look at the input:"]},{"cell_type":"code","metadata":{"id":"94K3oSYBDaF2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"2748a5ac-55c8-4098-b164-d7a7c3a83147","executionInfo":{"status":"ok","timestamp":1558270433025,"user_tz":-120,"elapsed":102976,"user":{"displayName":"Luka Tir","photoUrl":"","userId":"01311663910947370049"}}},"source":["plt.imshow(x_test[0])\n","plt.show()"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}